{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Wikipedia with Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The present notebook is a Py-Spark implementation of the 6th chapter of the book \"Advanced Analytics with Spark: Patterns for Learning from Data at Scale\" (Uri Laserson, Sean Owen, Sandy Ryza, Josh Wills), originally implemented in scala. The goal is to apply the LSA (Latent Semantic Analysis) algorithm to a corpus of Wikipedia articles. In order to do this, we employ the Wikipedia Data Dumps dataset (available<a href=https://dumps.wikimedia.org/> here</a>).\n",
    "\n",
    "LSA is a NLP techique which strives to find the relationship between words in a set of documents using relevant concepts. Using this technique we do not compare the words themselves, but the concepts attateched to these words. To solve this, we will use Linear Algebra techiques such as SVD and map words and documents to a new semantic space, and make concept comparisons in this space.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents <br>\n",
    "\n",
    "1. Pre Processing<br>\n",
    "1.1. Data Structures<br>\n",
    "1.1. Reading the dataset<br>\n",
    "2. TF-IDF<br>\n",
    "2.1. Term Frequency<br>\n",
    "2.2. Document-Term Frequency <br>\n",
    "2.3. Inverse-Document Frequency<br>\n",
    "2.4. Complete TF-IDF<br>\n",
    "2.5. Spark TF-IDF<br>\n",
    "3. LSA  (SVD)<br>\n",
    "4. Finding  Important Concepts <br>\n",
    "5. Term-Term Relevance<br>\n",
    "    5.1. Local Implementation<br>\n",
    "    5.2. Distributed Implementation<br>\n",
    "6. Document-Document Relevance<br>\n",
    "7. Term-Document Relevance<br>\n",
    "8. Multiple-Term Queries<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from operator import add\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow\n",
    "from sklearn.preprocessing import normalize\n",
    "from pyspark.mllib.linalg.distributed import *\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import *\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the maximum ammount of information from the text we need to pre-prcess the dataset. As so, we perform the following:<br>\n",
    "•\tRemove stop words and function words; <br>\n",
    "•\tRemove punctuation;<br>\n",
    "•\tRemove words that are too short (l <= 2);<br>\n",
    "•\tRemove nubers;<br>\n",
    "•\tPerform stemming (E.g., the words \"cat\" and \"cats\" will be considered the same word);<br>\n",
    "•\tRemove any trace of the original XML format of the dataset.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_chars = re.compile(\"[@$/#.ô€€€:&*+=\\\\[\\\\]?!()){},\\\\'\\\">_<;%\\\\s]+\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "function_words = \"a about above after again against ago ahead all almost along already also although always am among an and any are aren't around as at away backward backwards be because before behind below beneath beside between both but by can cannot can't cause 'cos could couldn't despite did didn't do does doesn't don't down during each either even ever every except for forward from had hadn't has hasn't have haven't he her here hers herself him himself his how however if in inside inspite instead into is isn't it its itself just 'll least less like 'm many may mayn't me might mightn't mine more most much must mustn't my myself near need needn't needs neither never no none nor not now of off often on once only onto or ought oughtn't our ours ourselves out outside over past perhaps quite 're rather 's seldom several shall shan't she should shouldn't since so some sometimes soon than that the their theirs them themselves then there therefore these they this those though through thus till to together too towards under unless until up upon us used usedn't usen't usually 've very was wasn't we well were weren't what when where whether which while who whom whose why will with without won't would wouldn't yet you your yours yourself yourselves\"\n",
    "\n",
    "function_words = function_words.split()\n",
    "\n",
    "junk_words = 'disambiguation article refer ref related onlyinclude'\n",
    "junk_words = junk_words.split()\n",
    "\n",
    "\n",
    "def remove_function_words(text):\n",
    "    return ' '.join([word for word in text.split() if word not in function_words])\n",
    "\n",
    "def remove_junk_words(text):\n",
    "    return ' '.join([word for word in text.split() if word not in junk_words])\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = gensim.parsing.preprocessing.strip_tags(text) \n",
    "    text = gensim.parsing.preprocessing.strip_punctuation(text)\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text) \n",
    "  \n",
    "    text = remove_function_words(text)\n",
    "    text = remove_junk_words(text)\n",
    "    \n",
    "    text = gensim.parsing.preprocessing.remove_stopwords(text) \n",
    "    \n",
    "    stemmer = gensim.parsing.porter.PorterStemmer()\n",
    "    text = stemmer.stem(text)\n",
    "    text = gensim.parsing.preprocessing.strip_short(text, minsize=3) \n",
    "  \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1. Used Data Structures \n",
    "\n",
    "RDDs (Resilient Distributed Datasets) are a better choice over DataFrames when: <br>\n",
    "* The data is not-structured (e.g., text data);\n",
    "* Data does not follow a schema, or if we don't need to access data by attribute (column or name);\n",
    "* It is necessary to apply data transformations and have low-level control over the data.\n",
    "\n",
    "As so, we structure the data using RDDs in this notebook. <a href=https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html>More information</a>\n",
    "\n",
    "###  1.2. Reading the dataset <br>\n",
    "\n",
    "The dataset is split in 24 files wich resulted from parsing the original wikipedia data dump using the <a href=https://github.com/attardi/wikiextractor>WikiExtractor Tool</a>. Each file is composed of numerous Wikipedia articles, delimited by html &lt;doc&gt; tags :<br>\n",
    "&lt;doc id=\"12\" url=\"https://en.wikipedia.org/wiki?curid=12\" title=\"Anarchism\"&gt;<br>\n",
    "Anarchism<br>\n",
    "Anarchism is a political philosophy that advocates (…) <br>\n",
    "&lt;doc&gt;<br>\n",
    "\n",
    "We use the method <b><i>newAPIHadoopFile()</i></b> in order to read the the articles to an RDD . This is simple as this method lets us configure the document format as XML, as well as the enclosing article tags.\n",
    "\n",
    "In the cell bellow we create 3 RDDs: <br></div>\n",
    "* doc_rdd: An RDD in which each line has the pre-processed text of each article;\n",
    "* doc_ids: An RDD in which each line has the ID of each article;\n",
    "* doc_titles: An RDD in which each line has the title of an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['australian white ibis australian white ibis threskiornis molucca wading bird ibis family threskiornithidae widespread australia predominantly white plumage bare black head long downcurved black legs sister species sacred ibis historically rare urban areas australian white ibis immigrated urban areas east coast increasing numbers late commonly seen wollongong sydney melbourne gold coast brisbane townsville recent years bird increasing common perth western australia surrounding towns south western australia populations disappeared natural breeding areas macquarie marshes north western new south wales management plans introduced control problematic urban populations sydney initially described georges cuvier ibis molucca considered superspecies complex sacred ibis aethiopicus africa black headed ibis melanocephalus asia status complex vacillated years older guidebooks referred bird species molucca comprehensive review plumage patterns holyoak holyoak noted species similarities australian taxon resembled aethiopicus adult plumage melanocephalus juvenile plumage proposed considered single species aethiopicus generally accepted scientific community lowe richards assessment plumage recommended recognition molucca species level followed chromosome study highlighted species having different karyotype australian white ibis considered species authorities alternate colloquial names include bin chicken dump chook trash vulture bankstown bin diver tip turkey habit rummaging garbage sheep bird known mardungurra yindjibarndi people central western pilbara subspecies recognised australian white ibis fairly large ibis species long bald black head neck long black downcurved beak measuring male female sexual dimorphism size slightly heavier male weighs compared female comparison american white ibis generally attains weight body plumage white brown stained inner secondary plumes displayed lacy black tail feathers upper tail yellow bird breeding legs feet dark red skin visible underside wing immature birds shorter bills head neck feathered juveniles white ibis gives foul stench smell described rotten odd smell unpleasant distinct long croak australian white ibis reaches sexual maturity years reach years age australian white ibis widespread eastern northern south western australia occurs marshy wetlands open grasslands common australian east coast city parks rubbish dumps urban areas wollongong sydney perth gold coast brisbane townsville historically rare urban areas influx noted drought drove birds eastwards late urban population increased period drought big colony set sydney suburb bankstown started anxiety local community estimated colony largest macquarie marshes natural breeding wetland inland nsw debate recent years consider pest possibly endangered species birds tourist areas sydney darling harbour royal botanic gardens centennial park problem strong smell populations areas culled birds come regarded problem species victoria result scavenging activities scattering rubbish tips bins process known snatch sandwiches picnickers behaviour propensity build nests inappropriate places competition captive animals led surplus birds relocated healesville sanctuary sale birds returned days macquarie marshes north western new south wales main areas breeding reported breeding pairs species absent tasmania australian white ibis range food includes terrestrial aquatic invertebrates human scraps favoured foods crayfish mussels bird obtains digging long breeding season varies location australia generally august november south february wet season north nest shallow dish shaped platform sticks grasses reeds located trees generally body water river swamp lake ibises commonly nest waterbirds egrets herons spoonbills cormorants dull white eggs laid measuring clutch incubated days hatchlings altricial naked helpless birth days fledg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
    "#read articles\n",
    "doc_rdd = sc.newAPIHadoopFile(\n",
    "    'hdfs://master:9000/home/ABD/datasets/ch6/wiki_02',\n",
    "    'com.databricks.spark.xml.XmlInputFormat',\n",
    "    'org.apache.hadoop.io.IntWritable',\n",
    "    'org.apache.hadoop.io.Text',\n",
    "    conf={\n",
    "        'xmlinput.start':'<doc>',\n",
    "        'xmlinput.end':'</doc>',\n",
    "        'xmlinput.encoding': 'utf-8'})\n",
    "\n",
    "doc_ids = doc_rdd.map(lambda x: x[1]).map(lambda x: x.split('doc id=\"')[1].split('\"')[0])\n",
    "doc_titles = doc_rdd.map(lambda x: x[1]).map(lambda x: x.split('title=\"')[1].split('\"')[0])\n",
    "\n",
    "doc_rdd = doc_rdd.map(lambda x: x[1]).map(lambda x: re.sub(r'\\<doc id=.+>', '', x)).\\\n",
    "    map(lambda x: re.sub(r'</doc>', '', x)).filter(lambda x: len(x) > 0).map(pre_process)\n",
    "    \n",
    "num_docs = doc_rdd.count()\n",
    "\n",
    "doc_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to calculate the dataset TF-IDF, which is a feature vectorization technique used in text mining problems to represent the importance of a term to a document in a text corpus. In a TF-IDF matrix, each line,  i, represents a term, each column, j, represents a document, and each position M(i, j) captures the importance of term i to document j. Tf-IDF also captures two intuitions: (i) if a term occurs many times in a document, the greater the importance of that term to that document, and (ii) it is more relevant to find a word in a document that occurs sparsely in a text corpus than to find a word that appears in a larger amount of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Term Frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Firstly, we count the occurrences of each term in each document. We end up with an RDD in which each entry is a dictionary of word occurrences for one article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'absent': 1,\n",
       "  'accepted': 1,\n",
       "  'activities': 1,\n",
       "  'adult': 1,\n",
       "  'aethiopicus': 3,\n",
       "  'africa': 1,\n",
       "  'age': 1,\n",
       "  'alternate': 1,\n",
       "  'altricial': 1,\n",
       "  'american': 1,\n",
       "  'animals': 1,\n",
       "  'anxiety': 1,\n",
       "  'aquatic': 1,\n",
       "  'areas': 8,\n",
       "  'asia': 1,\n",
       "  'assessment': 1,\n",
       "  'attains': 1,\n",
       "  'august': 1,\n",
       "  'australia': 5,\n",
       "  'australian': 10,\n",
       "  'authorities': 1,\n",
       "  'bald': 1,\n",
       "  'bankstown': 2,\n",
       "  'bare': 1,\n",
       "  'beak': 1,\n",
       "  'behaviour': 1,\n",
       "  'big': 1,\n",
       "  'bills': 1,\n",
       "  'bin': 2,\n",
       "  'bins': 1,\n",
       "  'bird': 6,\n",
       "  'birds': 6,\n",
       "  'birth': 1,\n",
       "  'black': 6,\n",
       "  'body': 2,\n",
       "  'botanic': 1,\n",
       "  'breeding': 6,\n",
       "  'brisbane': 2,\n",
       "  'brown': 1,\n",
       "  'build': 1,\n",
       "  'captive': 1,\n",
       "  'centennial': 1,\n",
       "  'central': 1,\n",
       "  'chicken': 1,\n",
       "  'chook': 1,\n",
       "  'chromosome': 1,\n",
       "  'city': 1,\n",
       "  'clutch': 1,\n",
       "  'coast': 4,\n",
       "  'colloquial': 1,\n",
       "  'colony': 2,\n",
       "  'come': 1,\n",
       "  'common': 2,\n",
       "  'commonly': 2,\n",
       "  'community': 2,\n",
       "  'compared': 1,\n",
       "  'comparison': 1,\n",
       "  'competition': 1,\n",
       "  'complex': 2,\n",
       "  'comprehensive': 1,\n",
       "  'consider': 1,\n",
       "  'considered': 3,\n",
       "  'control': 1,\n",
       "  'cormorants': 1,\n",
       "  'crayfish': 1,\n",
       "  'croak': 1,\n",
       "  'culled': 1,\n",
       "  'cuvier': 1,\n",
       "  'dark': 1,\n",
       "  'darling': 1,\n",
       "  'days': 3,\n",
       "  'debate': 1,\n",
       "  'described': 2,\n",
       "  'different': 1,\n",
       "  'digging': 1,\n",
       "  'dimorphism': 1,\n",
       "  'disappeared': 1,\n",
       "  'dish': 1,\n",
       "  'displayed': 1,\n",
       "  'distinct': 1,\n",
       "  'diver': 1,\n",
       "  'downcurved': 2,\n",
       "  'drought': 2,\n",
       "  'drove': 1,\n",
       "  'dull': 1,\n",
       "  'dump': 1,\n",
       "  'dumps': 1,\n",
       "  'east': 2,\n",
       "  'eastern': 1,\n",
       "  'eastwards': 1,\n",
       "  'eggs': 1,\n",
       "  'egrets': 1,\n",
       "  'endangered': 1,\n",
       "  'estimated': 1,\n",
       "  'fairly': 1,\n",
       "  'family': 1,\n",
       "  'favoured': 1,\n",
       "  'feathered': 1,\n",
       "  'feathers': 1,\n",
       "  'february': 1,\n",
       "  'feet': 1,\n",
       "  'female': 2,\n",
       "  'fledg': 1,\n",
       "  'followed': 1,\n",
       "  'food': 1,\n",
       "  'foods': 1,\n",
       "  'foul': 1,\n",
       "  'garbage': 1,\n",
       "  'gardens': 1,\n",
       "  'generally': 4,\n",
       "  'georges': 1,\n",
       "  'gives': 1,\n",
       "  'gold': 2,\n",
       "  'grasses': 1,\n",
       "  'grasslands': 1,\n",
       "  'guidebooks': 1,\n",
       "  'habit': 1,\n",
       "  'harbour': 1,\n",
       "  'hatchlings': 1,\n",
       "  'having': 1,\n",
       "  'head': 3,\n",
       "  'headed': 1,\n",
       "  'healesville': 1,\n",
       "  'heavier': 1,\n",
       "  'helpless': 1,\n",
       "  'herons': 1,\n",
       "  'highlighted': 1,\n",
       "  'historically': 2,\n",
       "  'holyoak': 2,\n",
       "  'human': 1,\n",
       "  'ibis': 16,\n",
       "  'ibises': 1,\n",
       "  'immature': 1,\n",
       "  'immigrated': 1,\n",
       "  'inappropriate': 1,\n",
       "  'include': 1,\n",
       "  'includes': 1,\n",
       "  'increased': 1,\n",
       "  'increasing': 2,\n",
       "  'incubated': 1,\n",
       "  'influx': 1,\n",
       "  'initially': 1,\n",
       "  'inland': 1,\n",
       "  'inner': 1,\n",
       "  'introduced': 1,\n",
       "  'invertebrates': 1,\n",
       "  'juvenile': 1,\n",
       "  'juveniles': 1,\n",
       "  'karyotype': 1,\n",
       "  'known': 2,\n",
       "  'lacy': 1,\n",
       "  'laid': 1,\n",
       "  'lake': 1,\n",
       "  'large': 1,\n",
       "  'largest': 1,\n",
       "  'late': 2,\n",
       "  'led': 1,\n",
       "  'legs': 2,\n",
       "  'level': 1,\n",
       "  'local': 1,\n",
       "  'located': 1,\n",
       "  'location': 1,\n",
       "  'long': 5,\n",
       "  'lowe': 1,\n",
       "  'macquarie': 3,\n",
       "  'main': 1,\n",
       "  'male': 2,\n",
       "  'management': 1,\n",
       "  'mardungurra': 1,\n",
       "  'marshes': 3,\n",
       "  'marshy': 1,\n",
       "  'maturity': 1,\n",
       "  'measuring': 2,\n",
       "  'melanocephalus': 2,\n",
       "  'melbourne': 1,\n",
       "  'molucca': 4,\n",
       "  'mussels': 1,\n",
       "  'naked': 1,\n",
       "  'names': 1,\n",
       "  'natural': 2,\n",
       "  'neck': 2,\n",
       "  'nest': 2,\n",
       "  'nests': 1,\n",
       "  'new': 2,\n",
       "  'north': 3,\n",
       "  'northern': 1,\n",
       "  'noted': 2,\n",
       "  'november': 1,\n",
       "  'nsw': 1,\n",
       "  'numbers': 1,\n",
       "  'obtains': 1,\n",
       "  'occurs': 1,\n",
       "  'odd': 1,\n",
       "  'older': 1,\n",
       "  'open': 1,\n",
       "  'pairs': 1,\n",
       "  'park': 1,\n",
       "  'parks': 1,\n",
       "  'patterns': 1,\n",
       "  'people': 1,\n",
       "  'period': 1,\n",
       "  'perth': 2,\n",
       "  'pest': 1,\n",
       "  'picnickers': 1,\n",
       "  'pilbara': 1,\n",
       "  'places': 1,\n",
       "  'plans': 1,\n",
       "  'platform': 1,\n",
       "  'plumage': 6,\n",
       "  'plumes': 1,\n",
       "  'population': 1,\n",
       "  'populations': 3,\n",
       "  'possibly': 1,\n",
       "  'predominantly': 1,\n",
       "  'problem': 2,\n",
       "  'problematic': 1,\n",
       "  'process': 1,\n",
       "  'propensity': 1,\n",
       "  'proposed': 1,\n",
       "  'range': 1,\n",
       "  'rare': 2,\n",
       "  'reach': 1,\n",
       "  'reaches': 1,\n",
       "  'recent': 2,\n",
       "  'recognised': 1,\n",
       "  'recognition': 1,\n",
       "  'recommended': 1,\n",
       "  'red': 1,\n",
       "  'reeds': 1,\n",
       "  'referred': 1,\n",
       "  'regarded': 1,\n",
       "  'relocated': 1,\n",
       "  'reported': 1,\n",
       "  'resembled': 1,\n",
       "  'result': 1,\n",
       "  'returned': 1,\n",
       "  'review': 1,\n",
       "  'richards': 1,\n",
       "  'river': 1,\n",
       "  'rotten': 1,\n",
       "  'royal': 1,\n",
       "  'rubbish': 2,\n",
       "  'rummaging': 1,\n",
       "  'sacred': 2,\n",
       "  'sale': 1,\n",
       "  'sanctuary': 1,\n",
       "  'sandwiches': 1,\n",
       "  'scattering': 1,\n",
       "  'scavenging': 1,\n",
       "  'scientific': 1,\n",
       "  'scraps': 1,\n",
       "  'season': 2,\n",
       "  'secondary': 1,\n",
       "  'seen': 1,\n",
       "  'set': 1,\n",
       "  'sexual': 2,\n",
       "  'shallow': 1,\n",
       "  'shaped': 1,\n",
       "  'sheep': 1,\n",
       "  'shorter': 1,\n",
       "  'similarities': 1,\n",
       "  'single': 1,\n",
       "  'sister': 1,\n",
       "  'size': 1,\n",
       "  'skin': 1,\n",
       "  'slightly': 1,\n",
       "  'smell': 3,\n",
       "  'snatch': 1,\n",
       "  'south': 5,\n",
       "  'species': 11,\n",
       "  'spoonbills': 1,\n",
       "  'stained': 1,\n",
       "  'started': 1,\n",
       "  'status': 1,\n",
       "  'stench': 1,\n",
       "  'sticks': 1,\n",
       "  'strong': 1,\n",
       "  'study': 1,\n",
       "  'subspecies': 1,\n",
       "  'suburb': 1,\n",
       "  'superspecies': 1,\n",
       "  'surplus': 1,\n",
       "  'surrounding': 1,\n",
       "  'swamp': 1,\n",
       "  'sydney': 5,\n",
       "  'tail': 2,\n",
       "  'tasmania': 1,\n",
       "  'taxon': 1,\n",
       "  'terrestrial': 1,\n",
       "  'threskiornis': 1,\n",
       "  'threskiornithidae': 1,\n",
       "  'tip': 1,\n",
       "  'tips': 1,\n",
       "  'tourist': 1,\n",
       "  'towns': 1,\n",
       "  'townsville': 2,\n",
       "  'trash': 1,\n",
       "  'trees': 1,\n",
       "  'turkey': 1,\n",
       "  'underside': 1,\n",
       "  'unpleasant': 1,\n",
       "  'upper': 1,\n",
       "  'urban': 6,\n",
       "  'vacillated': 1,\n",
       "  'varies': 1,\n",
       "  'victoria': 1,\n",
       "  'visible': 1,\n",
       "  'vulture': 1,\n",
       "  'wading': 1,\n",
       "  'wales': 2,\n",
       "  'water': 1,\n",
       "  'waterbirds': 1,\n",
       "  'weighs': 1,\n",
       "  'weight': 1,\n",
       "  'western': 6,\n",
       "  'wet': 1,\n",
       "  'wetland': 1,\n",
       "  'wetlands': 1,\n",
       "  'white': 13,\n",
       "  'widespread': 2,\n",
       "  'wing': 1,\n",
       "  'wollongong': 2,\n",
       "  'years': 5,\n",
       "  'yellow': 1,\n",
       "  'yindjibarndi': 1}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count(text):\n",
    "    unique_words = set(text)\n",
    "    words_list = list(unique_words)\n",
    "    res_dict = {}\n",
    "    for word in unique_words:\n",
    "        res_dict[word] = text.count(word)\n",
    "    return res_dict\n",
    "\n",
    "#term freq\n",
    "tf = doc_rdd.map(lambda x: count(x.split()))\n",
    "tf.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Document-Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The studied chapter proposes two solutions in order to calculate the document-term frequency (for each term, the number of documents in which it appears): <br>\n",
    "* <i>aggregate()</i>\n",
    "* <i>reduce()</i>\n",
    "\n",
    "In this notebook we use the reduce solution. The document-term frequencies are calculated in a distributed manner using <i>reduceByKey()</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#document term freq\n",
    "doc_term_freq = tf.flatMap(lambda x: x.keys()).map(lambda word: (word, 1)).reduceByKey(add)\n",
    "doc_term_freq.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to filter the less frequent terms, as the current number of terms is too high and many of those are useless since they only appear once in the entire corpus. We filter the terms and leave only the N more frequent terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[14] at parallelize at PythonRDD.scala:475"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TAKE TOP 50000 TERMS\n",
    "N = 50000\n",
    "doc_term_freq = sc.parallelize(doc_term_freq.top(N, key=lambda x: x[1]))\n",
    "doc_term_freq.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3. Inverse-Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IDF captures the importance of a term to all the documents - for instance, if a word occurs a large amount of times in a document, and also in other documents, maybe it is not an important word, only a frequent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idfs = doc_term_freq.map(lambda x: (x[0], math.log((num_docs / x[1]), 10)))\n",
    "l_idfs = idfs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to pass dictionary data to MLib the dict keys cannot be strings. Therefore we need to map each term to a numeric ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43923\n",
      "functionalism\n"
     ]
    }
   ],
   "source": [
    "term_ids = idfs.keys().zipWithIndex().collectAsMap()\n",
    "inverse_term_ids = idfs.keys().zipWithIndex().map(lambda x : (x[1], x[0])).collectAsMap()\n",
    "\n",
    "# keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. \n",
    "broadcasted_terms_ids = sc.broadcast(term_ids).value\n",
    "reverse_broadcasted_ter_ids = sc.broadcast(inverse_term_ids).value\n",
    "\n",
    "print(term_ids.get('functionalism'))\n",
    "print(inverse_term_ids.get(43923))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.4. Complete TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can now calculate the final TF-IDF RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(50000, {0: 0.0013, 2: 0.0015, 3: 0.0039, 15: 0.0011, 19: 0.0011, 20: 0.0058, 22: 0.0012, 26: 0.0012, 27: 0.0036, 35: 0.0012, 38: 0.0013, 40: 0.0064, 45: 0.0013, 46: 0.0027, 51: 0.0013, 59: 0.0014, 64: 0.0014, 69: 0.0014, 73: 0.0014, 76: 0.0043, 81: 0.0014, 84: 0.0014, 92: 0.0029, 94: 0.0015, 100: 0.0015, 101: 0.0015, 104: 0.0015, 107: 0.0031, 112: 0.0015, 117: 0.0016, 121: 0.0016, 138: 0.0049, 141: 0.0016, 143: 0.0033, 145: 0.0017, 148: 0.01, 151: 0.0217, 152: 0.0017, 153: 0.0017, 159: 0.0017, 162: 0.0017, 175: 0.0017, 179: 0.0052, 180: 0.0139, 185: 0.007, 190: 0.0018, 193: 0.0106, 194: 0.0018, 196: 0.0018, 211: 0.0018, 214: 0.0018, 226: 0.0018, 240: 0.0018, 244: 0.0037, 246: 0.0018, 253: 0.0019, 256: 0.0037, 266: 0.0019, 269: 0.0019, 274: 0.0019, 287: 0.0019, 289: 0.0019, 304: 0.0019, 306: 0.0019, 321: 0.0039, 328: 0.002, 336: 0.002, 342: 0.002, 347: 0.002, 365: 0.002, 380: 0.002, 391: 0.0041, 417: 0.0042, 423: 0.0042, 433: 0.0021, 473: 0.0021, 475: 0.0021, 485: 0.0043, 503: 0.0043, 513: 0.0022, 520: 0.0022, 538: 0.0044, 575: 0.0022, 577: 0.009, 578: 0.0022, 594: 0.0022, 596: 0.0022, 598: 0.0023, 644: 0.0023, 653: 0.0023, 660: 0.0023, 689: 0.0023, 690: 0.0023, 727: 0.0024, 737: 0.0024, 751: 0.0119, 756: 0.0262, 765: 0.0048, 782: 0.0024, 794: 0.0024, 799: 0.0024, 800: 0.0048, 813: 0.0048, 842: 0.0049, 948: 0.0025, 949: 0.0025, 951: 0.0025, 954: 0.0025, 955: 0.0025, 964: 0.0025, 965: 0.0151, 991: 0.0025, 1004: 0.0025, 1008: 0.0025, 1052: 0.0026, 1060: 0.0026, 1112: 0.0052, 1129: 0.0026, 1133: 0.0026, 1152: 0.0026, 1181: 0.0026, 1189: 0.0026, 1207: 0.0027, 1209: 0.0027, 1232: 0.0027, 1320: 0.0027, 1350: 0.0027, 1388: 0.0028, 1415: 0.0028, 1443: 0.0028, 1448: 0.0056, 1469: 0.0028, 1502: 0.0028, 1518: 0.0282, 1520: 0.0056, 1532: 0.0028, 1570: 0.0028, 1574: 0.0028, 1714: 0.0029, 1746: 0.0059, 1805: 0.0029, 1832: 0.003, 1836: 0.003, 1903: 0.003, 2003: 0.003, 2011: 0.003, 2036: 0.0182, 2057: 0.003, 2083: 0.0061, 2183: 0.0186, 2373: 0.0095, 2448: 0.0032, 2449: 0.0064, 2490: 0.0032, 2511: 0.0032, 2539: 0.0032, 2596: 0.0033, 2600: 0.0033, 2624: 0.0033, 2639: 0.0033, 2654: 0.0033, 2720: 0.0066, 2739: 0.0033, 2752: 0.0033, 2756: 0.0033, 2869: 0.0033, 2941: 0.0034, 2960: 0.0034, 3013: 0.0068, 3125: 0.0034, 3155: 0.0034, 3272: 0.0207, 3340: 0.0069, 3361: 0.0035, 3382: 0.0035, 3513: 0.0035, 3517: 0.0071, 3541: 0.0177, 3546: 0.0035, 3674: 0.0036, 3699: 0.0036, 3714: 0.0036, 3768: 0.0036, 3801: 0.0036, 3848: 0.0036, 4019: 0.0037, 4034: 0.0037, 4228: 0.0074, 4267: 0.0037, 4309: 0.0037, 4413: 0.0038, 4467: 0.0038, 4745: 0.0076, 5021: 0.0039, 5138: 0.0039, 5265: 0.0039, 5309: 0.004, 5334: 0.004, 5519: 0.004, 5586: 0.004, 5968: 0.0041, 5971: 0.0041, 6213: 0.0041, 6263: 0.0041, 6295: 0.0041, 6313: 0.0041, 6439: 0.0042, 6518: 0.0042, 6622: 0.0042, 6826: 0.0042, 6924: 0.0042, 6938: 0.0043, 6949: 0.0043, 7021: 0.0043, 7174: 0.0043, 7230: 0.0043, 7398: 0.0043, 7507: 0.0043, 7570: 0.0044, 7579: 0.0044, 7624: 0.0044, 7629: 0.0044, 8022: 0.0266, 8116: 0.0089, 8258: 0.0045, 8565: 0.009, 8628: 0.0045, 8854: 0.0045, 8930: 0.0046, 9001: 0.0091, 9166: 0.0046, 9249: 0.0138, 9360: 0.0092, 9374: 0.0046, 9412: 0.0046, 9461: 0.0046, 9476: 0.0046, 9495: 0.0139, 9662: 0.0047, 9751: 0.0047, 9794: 0.0047, 9911: 0.0047, 9938: 0.0047, 10441: 0.0048, 10885: 0.0048, 10940: 0.0048, 11069: 0.0048, 11246: 0.0048, 11424: 0.0049, 11708: 0.0049, 11751: 0.0049, 11846: 0.0049, 11921: 0.0049, 12483: 0.005, 12752: 0.005, 13044: 0.005, 13149: 0.005, 13488: 0.0051, 13518: 0.0051, 13710: 0.0051, 13775: 0.0051, 13782: 0.0051, 14413: 0.0052, 15120: 0.0052, 15154: 0.0052, 16603: 0.0054, 16988: 0.0054, 17085: 0.0054, 17154: 0.0054, 17558: 0.0055, 17876: 0.0055, 17962: 0.0055, 18524: 0.011, 18879: 0.0055, 18890: 0.0055, 20569: 0.0057, 20693: 0.0057, 21255: 0.0172, 21480: 0.0057, 21879: 0.0058, 22545: 0.0058, 22738: 0.0058, 24346: 0.0059, 24795: 0.0059, 25127: 0.006, 25918: 0.006, 26024: 0.006, 26922: 0.0061, 27686: 0.0061, 28289: 0.0061, 28503: 0.0062, 28601: 0.0062, 31228: 0.0063, 31231: 0.0063, 32064: 0.0063, 32197: 0.0063, 36745: 0.0131, 36770: 0.0065, 36824: 0.0065, 37422: 0.0131, 37546: 0.0066, 38195: 0.1053, 42751: 0.0068, 43160: 0.0068, 48569: 0.007})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tfidf(doc):\n",
    "    l = []\n",
    "    total_num_terms = sum(doc.values())\n",
    "    for key, value in doc.items():\n",
    "        if key in broadcasted_terms_ids:\n",
    "            term_id = broadcasted_terms_ids.get(key)\n",
    "        else:\n",
    "             continue\n",
    "        idf = l_idfs[term_id]\n",
    "        tf = value\n",
    "        #term tf-idf\n",
    "        res = idf[1] * tf / total_num_terms\n",
    "        l.append((term_id, res))\n",
    "        \n",
    "    return SparseVector(len(l_idfs), l)\n",
    "        \n",
    "\n",
    "tf_idf = tf.map(get_tfidf)\n",
    "tf_idf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Additionally, we can query this structure. Given a search term, a list with the highest valued TF-IDF for that term will be retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.36996278515018993, 'Lincoln University'), (0.36996278515018993, 'University of Leuven'), (0.36996278515018993, 'University Heights'), (0.29597022812015195, 'University Park'), (0.26906384374559267, 'University College'), (0.24664185676679329, 'Lingnan University'), (0.24664185676679329, 'University of New England'), (0.22197767109011396, 'Taiwan University System'), (0.22197767109011396, 'University System of Formosa'), (0.22197767109011396, 'Jiaotong University')]\n",
      "399.71733713150024s\n"
     ]
    }
   ],
   "source": [
    "def query_tfidf(term):\n",
    "\n",
    "    term_id = broadcasted_terms_ids.get(term)\n",
    "\n",
    "    # extract the TF*IDF score for the term's id into\n",
    "    # a new RDD for each document:\n",
    "    term_relevance = tf_idf.map(lambda x: x[term_id])\n",
    "    # zip with the document names so we can see which is which:\n",
    "    zippedResults = term_relevance.zip(doc_titles)\n",
    "    \n",
    "    return zippedResults.top(10, lambda x: x[0])\n",
    "\n",
    "start = time.time()\n",
    "print(query_tfidf('university'))\n",
    "print(str(time.time() - start) +'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Spark TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In alternative to the previous code, Spark has an <a href=https://spark.apache.org/docs/2.0.2/mllib-feature-extraction.html>TF-IDF implementation</a> which resorts to a hashing trick, in which each feature is mapped to a index by applying a hashing function. However, it is not possible to: (ii) compute the inverse transform (from indices to strings) and (ii) there can be collisions (multiple terms mapped to the same index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps a sequence of terms to their term frequencies using the hashing trick.\n",
    "# Now hash the words in each document to their term frequencies:\n",
    "hashingTF = HashingTF()\n",
    "hashing_tf = hashingTF.transform(doc_rdd)\n",
    "hashing_tf.cache()\n",
    "\n",
    "# Computes the inverse document frequency.\n",
    "hashing_idf = IDF().fit(hashing_tf)\n",
    "hashing_tfidf = hashing_idf.transform(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LSA  (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SVD (<i>Singular Value Decomposition</i>) is a matrix decomposition algorithm. Given a matrix m \\times n, the algorithm produces three matrices, which are approximately equal to the first one when multiplied:\n",
    "\n",
    "\\begin{equation*}\n",
    "M=USV^T,\n",
    "\\end{equation*}\n",
    "\n",
    "<p> where, U is an m × k matrix, in which each row represents a document and each column a concept, S is a k × k diagonal matrix, in which each entry corresponds to the strength of a concept, and V^T is a k × n matrix in which each column represents a term and each row a concept.\n",
    "In the text mining context, SVD is called LSA (Latent Semantic Analysis).\n",
    "\n",
    "At this moment there are no PySpark implementations of the computeSVD method, which is available only for the Scala and Java APIs. We use the solution proposed <a href=https://stackoverflow.com/a/33500704/3415409>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SVD(JavaModelWrapper):\n",
    "    \"\"\"Wrapper around the SVD scala case class\"\"\"\n",
    "    @property\n",
    "    def U(self):\n",
    "        \"\"\" Returns a RowMatrix whose columns are the left singular vectors of the SVD if computeU was set to be True.\"\"\"\n",
    "        u = self.call(\"U\")\n",
    "        if u is not None:\n",
    "            return RowMatrix(u)\n",
    "\n",
    "    @property\n",
    "    def s(self):\n",
    "        \"\"\"Returns a DenseVector with singular values in descending order.\"\"\"\n",
    "        return self.call(\"s\")\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\" Returns a DenseMatrix whose columns are the right singular vectors of the SVD.\"\"\"\n",
    "        return self.call(\"V\")\n",
    "    \n",
    "def computeSVD(row_matrix, k, computeU=False, rCond=1e-9):\n",
    "    \"\"\"\n",
    "    Computes the singular value decomposition of the RowMatrix.\n",
    "    The given row matrix A of dimension (m X n) is decomposed into U * s * V'T where\n",
    "    * s: DenseVector consisting of square root of the eigenvalues (singular values) in descending order.\n",
    "    * U: (m X k) (left singular vectors) is a RowMatrix whose columns are the eigenvectors of (A X A')\n",
    "    * v: (n X k) (right singular vectors) is a Matrix whose columns are the eigenvectors of (A' X A)\n",
    "    :param k: number of singular values to keep. We might return less than k if there are numerically zero singular values.\n",
    "    :param computeU: Whether of not to compute U. If set to be True, then U is computed by A * V * sigma^-1\n",
    "    :param rCond: the reciprocal condition number. All singular values smaller than rCond * sigma(0) are treated as zero, where sigma(0) is the largest singular value.\n",
    "    :returns: SVD object\n",
    "    \"\"\"\n",
    "    java_model = row_matrix._java_matrix_wrapper.call(\"computeSVD\", int(k), computeU, float(rCond))\n",
    "    return SVD(java_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The code below creates a RowMatrix using de TF-IDF matrix and passes is to the computeSVD method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2293.676204919815s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# 100 ?\n",
    "NUM_CONCEPTS = 50\n",
    "\n",
    "tf_idf.cache()\n",
    "mat = RowMatrix(tf_idf)\n",
    "\n",
    "svd = computeSVD(mat, NUM_CONCEPTS, True)\n",
    "\n",
    "print(str(time.time() - start) + 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain each of the three matrices. Note that the U and V matrices are stored in memory locally, while V is a distributed matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.linalg.distributed.RowMatrix'>\n",
      "<class 'pyspark.mllib.linalg.DenseVector'>\n",
      "<class 'pyspark.mllib.linalg.DenseMatrix'>\n"
     ]
    }
   ],
   "source": [
    "u = svd.U # rows.collect()\n",
    "s = svd.s\n",
    "v = svd.V\n",
    "\n",
    "print(type(u))\n",
    "print(type(s))\n",
    "print(type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rest of this notebook, we make distributed computations using MLib and the equivalent local computations using numpy arrays. The code bellow transforms the matrices to the formats used in latter cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distributed\n",
    "S_distributed =  sc.parallelize(np.diag(s.toArray())).zipWithIndex()\n",
    "V_distributed =  sc.parallelize(v.toArray()).zipWithIndex()\n",
    "S_distributed = IndexedRowMatrix( \\\n",
    "    S_distributed \\\n",
    "    .map(lambda row: IndexedRow(row[1], row[0])) \\\n",
    "    ).toBlockMatrix()\n",
    "\n",
    "\n",
    "V_distributed = IndexedRowMatrix( \\\n",
    "    V_distributed \\\n",
    "    .map(lambda row: IndexedRow(row[1], row[0])) \\\n",
    "    ).toBlockMatrix()\n",
    "\n",
    "\n",
    "SV_distributed = V_distributed.multiply(S_distributed)\n",
    "\n",
    "# local\n",
    "V = v.toArray()\n",
    "S = np.diag(s.toArray())\n",
    "\n",
    "SV = np.dot(V, S)\n",
    "SV_normalized = normalize(SV, 'l2')\n",
    "\n",
    "aux = u.rows.map(lambda row: row.toArray())\n",
    "U = np.array(np.array(aux.collect()))\n",
    "\n",
    "US = np.dot(U, S)\n",
    "US_normalized = normalize(US, 'l2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Finding  Important Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to check the terms and documents relevant to each of the concepts.\n",
    "\n",
    "<b>top_terms_in_concepts(N)</b>: Returns the most relevant terms for the first N concepts. As the V matrix is local these computations are performed locally.\n",
    "<b>top_docs_in_concepts(N)</b>: Returns the most relevant documents for the first N concepts. As the U matrix (which corresponds to the relationship between documents and concepts) is distributed these computations are performed in a distributed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local\n",
    "def top_terms_in_concepts(num_results):\n",
    "    top_terms = []\n",
    "    \n",
    "    for compNum in range(num_results):\n",
    "        comp = V.T[compNum]\n",
    "        # Sort the weights in the first component, and get the indeces\n",
    "        indeces = np.argsort(comp).tolist()\n",
    "        # Reverse the indeces, so we have the largest weights first.\n",
    "        indeces.reverse()\n",
    "        terms = []\n",
    "        weights = []\n",
    "        for i in indeces[:num_results]:\n",
    "            term = reverse_broadcasted_ter_ids.get(i) \n",
    "            terms.append(term)\n",
    "        weights = comp[indeces[:10]]\n",
    "        res = list(zip(terms, weights))\n",
    "        top_terms.append(res)\n",
    "        \n",
    "    return top_terms\n",
    "\n",
    "# distributed\n",
    "def top_docs_in_concepts(num_results):\n",
    "    top_docs = []\n",
    "    for num in range(num_results):\n",
    "        doc_weights = u.rows.map(lambda i : i.toArray())\n",
    "        doc_weights_for_concept = doc_weights.map(lambda i : i[num]).zip(doc_titles)\n",
    "        top_docs_concept = doc_weights_for_concept.top(num_results, lambda i : i[0])\n",
    "        top_docs.append(top_docs_concept)\n",
    "        \n",
    "    return top_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept terms: \n",
      "[('countout', -1.4197141405214752e-07), ('chromed', -1.5358158327644816e-07), ('rearm', -1.5507924207199048e-07), ('teleporting', -1.6619557592672089e-07), ('unmask', -1.7496211247549191e-07), ('teleported', -1.788225940199725e-07), ('starches', -1.7981636203898323e-07), ('armband', -1.9541219844880322e-07), ('concussive', -1.9599618682463553e-07), ('reassessed', -1.9968197834944072e-07)]\n",
      "\n",
      "Concept Docs\n",
      "[(0.0, 'Omoikane'), (0.0, 'Naxi'), (0.0, 'Tojo'), (0.0, 'Nikolaes Heinsius'), (0.0, 'Morus'), (0.0, 'UWC'), (0.0, '2060'), (0.0, 'Uniontown'), (0.0, 'Phylactery'), (0.0, 'HSP')]\n",
      "\n",
      "------------------------------------\n",
      "Concept terms: \n",
      "[('rowspan', 0.047212926855510304), ('senators', 0.0031371715317751723), ('senator', 0.0017092443780995603), ('class', 0.0014898894365866476), ('die', 0.0011524894199377266), ('elects', 0.00037413143165120278), ('republicans', 0.0002520801338609358), ('admitted', 0.0001888174181935986), ('recent', 0.000166364552987889), ('secession', 0.00011055485127424954)]\n",
      "\n",
      "Concept Docs\n",
      "[(0.0092222109381264808, 'List of United States Senators from Ohio'), (0.0091994648564455699, 'List of United States Senators from South Carolina'), (0.0091147084987602042, 'List of United States Senators from Tennessee'), (0.0090915161673538365, 'List of United States Senators from Louisiana'), (0.0090220976729226358, 'List of United States Senators from North Carolina'), (0.0083442307421692889, 'List of United States Senators from New Hampshire'), (0.0083112827763216161, 'List of United States Senators from Kentucky'), (0.0081952396145586019, 'List of United States Senators from New Jersey'), (0.0079510316625036326, 'List of United States Senators from Delaware'), (0.0077995238605813866, 'List of United States Senators from Iowa')]\n",
      "\n",
      "------------------------------------\n",
      "Concept terms: \n",
      "[('music', 0.54618179213448825), ('rowspan', 0.016302542496534803), ('mus', 0.0050762306823655462), ('macedonia', 0.004439447813330109), ('andalusian', 0.0036476765806296172), ('grammer', 0.0034017530690185621), ('loaded', 0.0032136299422411856), ('album', 0.0027259972599933957), ('video', 0.0022199290204558708), ('dominican', 0.0021649421503998206)]\n",
      "\n",
      "Concept Docs\n",
      "[(0.061930283382799697, '1845 in music'), (0.061930283382799697, '1859 in music'), (0.061930283382799697, '1858 in music'), (0.061930283382799697, '1857 in music'), (0.061930283382799697, '1856 in music'), (0.061930283382799697, '1855 in music'), (0.061930283382799697, '1854 in music'), (0.061930283382799697, '1853 in music'), (0.061930283382799697, '1852 in music'), (0.061930283382799697, '1851 in music')]\n",
      "\n",
      "------------------------------------\n",
      "Concept terms: \n",
      "[('canada', 0.69678042942062901), ('events', 0.26531954870280039), ('music', 0.055787510775060251), ('year', 0.046224762414991724), ('india', 0.01680864382162911), ('art', 0.016806870060471667), ('united', 0.0084462960308916042), ('canadian', 0.0081890942973175986), ('awards', 0.0076967969635644553), ('newfoundland', 0.0076827867369367716)]\n",
      "\n",
      "Concept Docs\n",
      "[(0.055818897971864338, '1867 in Canada'), (0.055818897971864338, '1868 in Canada'), (0.055818897971864338, '1869 in Canada'), (0.055818897971864338, '1870 in Canada'), (0.055818897971864338, '1871 in Canada'), (0.055818897971864338, '1872 in Canada'), (0.055818897971864338, '1873 in Canada'), (0.055818897971864338, '1874 in Canada'), (0.055818897971864338, '1875 in Canada'), (0.055818897971864338, '1876 in Canada')]\n",
      "\n",
      "------------------------------------\n",
      "Concept terms: \n",
      "[('science', 0.0079855586873071871), ('significant', 0.0047594253221341504), ('involved', 0.0047501110949833072), ('technology', 0.002218208361641022), ('year', 0.0017253861863139753), ('list', 0.0013248880985567008), ('events', 0.00074415306030013728), ('music', 0.00048365711414905671), ('authors', 0.00032645027702928563), ('rowspan', 0.00019830077699378897)]\n",
      "\n",
      "Concept Docs\n",
      "[(0.00074333102600648143, '1643 in science'), (0.00074333102600648143, '1727 in science'), (0.00074333102600648143, '1686 in science'), (0.00074333102600648143, '1779 in science'), (0.00074333102600648143, '1999 in science'), (0.00074333102600648143, '2002 in science'), (0.00074333102600648143, '1602 in science'), (0.00074333102600648143, '2000 in science'), (0.00074333102600648143, '1603 in science'), (0.00074333102600648143, '1604 in science')]\n",
      "\n",
      "------------------------------------\n",
      "Concept terms: \n",
      "[('science', 0.0024103780229846328), ('technology', 0.0013815189273274908), ('music', 0.0012581184206548746), ('involved', 0.0010291633567781899), ('significant', 0.0010015958992185409), ('year', 0.00054858441768266403), ('canada', 0.00031094509227142225), ('rowspan', 0.00029458942516126518), ('stand', 0.00013581559774680785), ('events', 0.00010779101120809292)]\n",
      "\n",
      "Concept Docs\n",
      "[(0.00024514071597016502, 'SX'), (0.0002300684479331701, '1643 in science'), (0.0002300684479331701, '1727 in science'), (0.0002300684479331701, '1686 in science'), (0.0002300684479331701, '1779 in science'), (0.0002300684479331701, '1999 in science'), (0.0002300684479331701, '2002 in science'), (0.0002300684479331701, '1602 in science'), (0.0002300684479331701, '2000 in science'), (0.0002300684479331701, '1603 in science')]\n",
      "\n",
      "------------------------------------\n",
      "Concept terms: \n",
      "[('science', 0.16072195074606066), ('technology', 0.08898984152699177), ('music', 0.087421424855782734), ('involved', 0.066330534539673872), ('significant', 0.064661718535227425), ('year', 0.037450021213398813), ('canada', 0.025558219141549123), ('rowspan', 0.020606198587199268), ('literatur', 0.013528943104870184), ('events', 0.0051339190221152235)]\n",
      "\n",
      "Concept Docs\n",
      "[(0.016039502131078884, 'SX'), (0.0153924872382116, '1643 in science'), (0.0153924872382116, '1727 in science'), (0.0153924872382116, '1686 in science'), (0.0153924872382116, '1779 in science'), (0.0153924872382116, '1999 in science'), (0.0153924872382116, '2002 in science'), (0.0153924872382116, '1602 in science'), (0.0153924872382116, '2000 in science'), (0.0153924872382116, '1603 in science')]\n",
      "\n",
      "------------------------------------\n",
      "Concept terms: \n",
      "[('uss', 0.95857071505817726), ('navy', 0.10385289496140908), ('ships', 0.073622015566447874), ('maddox', 0.055344919791541609), ('adirondack', 0.051120996897147893), ('intrepid', 0.040512142220253768), ('states', 0.039305809720996163), ('hms', 0.03879968165715892), ('united', 0.038787392567327696), ('eel', 0.0373794542783508)]\n",
      "\n",
      "Concept Docs\n",
      "[(0.30027448739085394, 'USS'), (0.16404854337076782, 'USS Maddox'), (0.16246869625869242, 'USS Adirondack'), (0.16034555565702649, 'USS Intrepid'), (0.15866910584024344, 'USS Eel'), (0.15717096852660806, 'USS Pueblo'), (0.1568946783023934, 'USS Randolph'), (0.15667884123635645, 'USS Savannah'), (0.15649119953355567, 'USS Raleigh'), (0.15630999299952289, 'USS Concord')]\n",
      "\n",
      "------------------------------------\n",
      "Concept terms: \n",
      "[('ref', 0.73936369010995717), ('olympics', 0.66515551084517222), ('paralympics', 0.085242022051335783), ('tang', 0.033973777065397129), ('zelda', 0.030139008315831047), ('newgate', 0.019073947553390234), ('follow', 0.012205007952278857), ('automatic', 0.010634129517804538), ('painting', 0.010125829668019982), ('commonly', 0.010023416476596178)]\n",
      "\n",
      "Concept Docs\n",
      "[(0.27953310687803679, '1952 Olympics'), (0.27953310687803679, '1956 Olympics'), (0.27953310687803679, '1960 Olympics'), (0.27953310687803679, '1964 Olympics'), (0.27953310687803679, '1968 Olympics'), (0.27953310687803679, '1972 Olympics'), (0.27953310687803679, '1976 Olympics'), (0.27953310687803679, '1984 Olympics'), (0.27953310687803679, '1988 Olympics'), (0.16980088660191522, '1976 Paralympics')]\n",
      "\n",
      "------------------------------------\n",
      "Concept terms: \n",
      "[('stand', 0.0098884256967642983), ('list', 0.0048209409997212677), ('authors', 0.0021077410604657338), ('uss', 0.0011904561258473528), ('aviation', 0.00056719482800909285), ('iata', 0.00038094583381575369), ('municipalities', 0.00035152314186346072), ('tcs', 0.00033722714315501741), ('omb', 0.00033672799172615225), ('ccr', 0.00032437277310848908)]\n",
      "\n",
      "Concept Docs\n",
      "[(0.0021494544801407929, 'BW'), (0.0021494544801407929, 'GS'), (0.0021494544801407929, 'CW'), (0.0021494544801407929, 'Rb'), (0.0021494544801407929, 'PY'), (0.0021494544801407929, 'TG'), (0.0021494544801407929, 'GC'), (0.0021494544801407929, 'SW'), (0.0021494544801407929, 'FIRE'), (0.0021494544801407929, 'DQ')]\n",
      "\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# num results\n",
    "N = 10\n",
    "top_concept_terms = top_terms_in_concepts(N)\n",
    "top_concept_docs = top_docs_in_concepts(N)\n",
    "\n",
    "for i in range(N):\n",
    "    print('Concept terms: ')\n",
    "    print(top_concept_terms[i])\n",
    "    print('\\nConcept Docs')\n",
    "    print(top_concept_docs[i])\n",
    "    print('\\n------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Term-Term Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA interprets the relationship between two terms as the cosine similarity between the columns corresponding to these terms in the rebuilt matrix. Additionally, it provides a more useful data representation as, (i) in considers word synonyms, (ii) considers polysemies, (iii) removes noise.\n",
    "\n",
    "The cosine similarity between two columns in the rebuilt matrix is equal to the cosine similarity between two columns in the SV^t matrix. Therefore, in order to find the most relevant terms to a certain term, we normalize de SV matrix and multiply it by the matrix line which corresponds to the term. Each entry in the resulting array will be the cosine similarity between that term and all the other terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.  Local Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the relevant_terms function in two ways. We need two structures: the normalized SV matrix and the column of the normalized SV matrix which corresponds to the current term so that we can multiply them and get an array with the cosine similarities between the current term and all the other terms.\n",
    "\n",
    "We implement two functions that perform the same job in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Método 1:\n",
      "[('obama', 0.99999999999999978), ('nativist', 0.99908273401611714), ('oversight', 0.99890863500119087), ('medicaid', 0.99886963690024222), ('hillary', 0.99879282507608658), ('vouchers', 0.99877638001720426), ('barack', 0.99877066093904077), ('allegiances', 0.99876433677028809), ('watchdog', 0.99872863379154408), ('congresswoman', 0.99868737406575148)]\n",
      "-----------\n",
      "Método 2:\n",
      "[('obama', 0.99999999999999989), ('nativist', 0.99908273401611736), ('oversight', 0.99890863500119131), ('medicaid', 0.99886963690024244), ('hillary', 0.9987928250760868), ('vouchers', 0.99877638001720459), ('barack', 0.99877066093904099), ('allegiances', 0.99876433677028842), ('watchdog', 0.99872863379154431), ('congresswoman', 0.9986873740657517)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def relevant_terms(term):\n",
    "    try:\n",
    "        index = broadcasted_terms_ids.get(term)\n",
    "    except:\n",
    "        print(\"Term doesn't exist\")\n",
    "        \n",
    "    cosine_sim = np.dot(SV_normalized, SV_normalized[index])\n",
    "    indeces = np.argsort(cosine_sim).tolist()\n",
    "    indeces.reverse()\n",
    "    \n",
    "    terms = []\n",
    "    for i in indeces[:10]:\n",
    "        related_term = inverse_term_ids.get(i) \n",
    "        terms.append(related_term)\n",
    "    return(list(zip(terms, cosine_sim[indeces])))\n",
    "    \n",
    "\n",
    "    \n",
    "def relevant_terms2(term): \n",
    "    try:\n",
    "        index = broadcasted_terms_ids.get(term)\n",
    "    except:\n",
    "        print('Term does not exist')\n",
    "        return\n",
    "    \n",
    "    #vector with one non-zero entry (term idf)\n",
    "    term_vector = np.zeros((len(l_idfs),))\n",
    "    term_vector[index] = l_idfs[index][1]\n",
    "    \n",
    "    #get original row from V\n",
    "    row = np.dot(term_vector, V)\n",
    "    #get row multiplied by S\n",
    "    row = np.dot(row, S)\n",
    "\n",
    "    row = normalize([row], 'l2') \n",
    "    cosine_sim = np.dot(SV_normalized, row.reshape(NUM_CONCEPTS,))\n",
    "    \n",
    "    indeces = np.argsort(cosine_sim).tolist()\n",
    "    indeces.reverse()\n",
    "    \n",
    "    terms = []\n",
    "    for i in indeces[:10]:\n",
    "        related_term = inverse_term_ids.get(i) \n",
    "        terms.append(related_term)\n",
    "        \n",
    "    return(list(zip(terms, cosine_sim[indeces])))\n",
    "\n",
    "print('Método 1:')\n",
    "print(relevant_terms('obama'))\n",
    "print('-----------')\n",
    "print('Método 2:')\n",
    "print(relevant_terms2('obama'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Distributed Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section exists only as an example to demonstrate that we can get the most relevant terms for a term in a distributed manner, using different Mlib matrix types. Mlib provides two general matrix types: local and distributed matrices. There are two implementations of local matrices (DenseMatrix and SparseMatrix) and four implementations of distributed matrices  (CoordinateMatrix,  IndexedRowMatrix, RowMatrix e BlockMatrix). Out of all the matrix implementations, the only one that supports multiplication operations is the BlockMatrix. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, in order to perform these computations in a distributed manner, we have to perform the following operations:<br>\n",
    "* Find the index of the term passed as an argument;\n",
    "* Normalize the SV matrix: this is a problem as we need SV to be a BlockMAtrix -- as this is the only type that allows matrix multiplication--, and these matrices have no \"rows\" attribute (only a \"blocks\" attribute) that allow us to access to the matrix rows and do modifications in a distributed manner. Therefore, we need to:\n",
    "* convert this matrix into an IndexedRowMatrix (a collection of IndexedRows where each IndexedRow is a tuple with the line index and an array);\n",
    "* Normalize this matrix; The result is an RDD;\n",
    "* Transform the result into a BlockMatrix, as this is the only matrix that allows us to perform multiplication operations;\n",
    "* Find the term array (i.e., the array that corresponds to the term, which is a row of the SV matrix and has the weights of each concept to that term);\n",
    "* Transform the array into a BlockMatrix;\n",
    "* Compute the cosine similarity of this term with all other terms by multiplying the term array by the normalized SV matrix. The result is a BlockMatrix.\n",
    "* Transform the cosine similarity BlockMatrix into an IndexedRowMatrix in order to access the rows;\n",
    "* Order the values and return the N most similar.\n",
    "\n",
    "<br>\n",
    "This is a huge work-around and therefore the rest of this notebook is implemented to run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('obama', 0.99999999999999956),\n",
       " ('nativist', 0.99908273401611747),\n",
       " ('oversight', 0.9989086350011912),\n",
       " ('medicaid', 0.99886963690024222),\n",
       " ('hillary', 0.99879282507608647),\n",
       " ('vouchers', 0.99877638001720448),\n",
       " ('barack', 0.99877066093904088),\n",
       " ('allegiances', 0.99876433677028809),\n",
       " ('watchdog', 0.99872863379154408),\n",
       " ('congresswoman', 0.99868737406575148)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_distributed_matrix(row):\n",
    "    index = row.index\n",
    "    vector = row.vector\n",
    "    v = DenseVector(normalize(vector, 'l2').reshape(NUM_CONCEPTS,))\n",
    "    return (v, index)  \n",
    "\n",
    "\n",
    "def relevant_terms_distributed(term, N):\n",
    "    \n",
    "    try:\n",
    "        index = broadcasted_terms_ids.get(term)\n",
    "    except:\n",
    "        print('Term does not exist')\n",
    "        return\n",
    "    \n",
    "    # normalize SV\n",
    "    SV_rdd = SV_distributed.toIndexedRowMatrix().rows.map(normalize_distributed_matrix).sortBy(lambda x: x[1])\n",
    "    \n",
    "    #transform SV back to a BlockMatrix\n",
    "    SV_normalized = IndexedRowMatrix( \\\n",
    "        SV_rdd \\\n",
    "        .map(lambda row:  IndexedRow(row[1], row[0])) \\\n",
    "        ).toBlockMatrix()\n",
    "    \n",
    "    # get term Array (index term)\n",
    "    term_array = SV_normalized.toIndexedRowMatrix().rows.filter(lambda r: r.index == index).\\\n",
    "        map(lambda x: x.vector).first()\n",
    "   \n",
    "    # transform term array to a Block Matrix\n",
    "    term_array = sc.parallelize([term_array.toArray()])\n",
    "\n",
    "    term_array = term_array.zipWithIndex()\n",
    "\n",
    "    term_array = IndexedRowMatrix(term_array \\\n",
    "            .map(lambda row: IndexedRow(row[1], row[0]))).toBlockMatrix()\n",
    "    # Multiply SV by term:array\n",
    "    cosine_sim = SV_normalized.multiply(term_array.transpose())\n",
    "    a = cosine_sim.toIndexedRowMatrix().rows.top(N, lambda x: x.vector[0])\n",
    "    \n",
    "    results = []\n",
    "    for ele in a:\n",
    "        results.append((inverse_term_ids.get(ele.index), ele.vector[0]))\n",
    "    return results\n",
    "\n",
    "relevant_terms_distributed('obama', 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6. Document-Document Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can employ the technique from the previous section in order to compute the document-document relevancies. Let u1 be a feature vector for document 1, in order to find the similarity between this document and the remaining, we need to perform the operation (US) × u1, where US is normalized. The remaining computations are the same as in section 5.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relevant_docs(doc):\n",
    "    \n",
    "    try:\n",
    "        index = np.where(np_titles == doc)[0][0]\n",
    "        #print(index)\n",
    "    except IndexError:\n",
    "        print('No such document')\n",
    "        return\n",
    "    \n",
    "    cosine_sim = np.dot(US_normalized, US_normalized[index])\n",
    "\n",
    "    indeces = np.argsort(cosine_sim).tolist()\n",
    "    indeces.reverse()\n",
    "    \n",
    "    print(list(zip(np_titles[indeces[:10]], cosine_sim[indeces])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Modulation (music)', 1.0000000000000002), ('Dynamic range compression', 0.99411443174973124), ('Mystic chord', 0.99374833542782792), ('Thirteenth', 0.99281379406060255), ('Gudok', 0.99265466923047119), ('Semitone', 0.99262905480767816), ('Secondary dominant', 0.99226633118166496), ('Jostein Hasselgård', 0.99165677191222212), ('Ring modulation', 0.99147842907840711), ('Enrique Iglesias', 0.99140837634422774)]\n"
     ]
    }
   ],
   "source": [
    "np_titles = np.array(doc_titles.collect())\n",
    "relevant_docs('Modulation (music)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Term-Document Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous sections, if v1 is the feature vector for a term, we can obtain the similarity vector between this term and the remaining by performing the operation (US) × v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topDocsForTerm(term):\n",
    "    \n",
    "    try:\n",
    "        index = broadcasted_terms_ids.get(term)\n",
    "        term_row = V[index]\n",
    "    except:\n",
    "        print(\"Term doesn't exist\")\n",
    "        return\n",
    "    \n",
    "    cosine_sim = np.dot(U, np.dot(S, term_row))\n",
    "    indeces = np.argsort(cosine_sim).tolist()\n",
    "    indeces.reverse()\n",
    "    \n",
    "    return list(zip(np_titles[indeces[:10]], cosine_sim[indeces]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Partial', 0.00079606926244068794), ('Republican Party', 0.00065511300383191127), ('Unionist Party', 0.00064655768542568244), ('Progressive Party', 0.00064260817895718362), ('Union Party', 0.00064004578222817091), ('Liberty Party', 0.00061416126895609167), ('Freedom Party', 0.00060772873133703182), ('New Party', 0.00060357535639102679), ('American Party', 0.0006018677331747779), ('Democratic Socialist Party', 0.00059126595399951448)]\n",
      "----\n",
      "[('1899 in Canada', 0.51613812534439163), ('1890 in Canada', 0.51613812534439163), ('1882 in Canada', 0.51613812534439163), ('1884 in Canada', 0.51613812534439163), ('1977 in Canada', 0.51613812534439163), ('1944 in Canada', 0.51613812534439163), ('1933 in Canada', 0.51613812534439163), ('1894 in Canada', 0.51613812534439163), ('1956 in Canada', 0.51613812534439163), ('1891 in Canada', 0.51613812534439163)]\n"
     ]
    }
   ],
   "source": [
    "print(topDocsForTerm('obama'))\n",
    "print('----')\n",
    "print(topDocsForTerm('canada'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multiple-Term Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to find the relevance from one term to all the others is to generate an array, set the value at the index of that term to 1 and multiply that array by V. We can escalate this method to handle multiple terms: the only alteration is that here, we set two entries in the array to 1. In order to maintain the weights of the terms, the value set at the index of the terms will not be one, but the IDF value for each of the terms.\n",
    "Then, to calculate the similarities we multiply the resulting array by S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topDocsFromTermQuery(term1, term2):\n",
    "    \n",
    "    try:\n",
    "        index1 = broadcasted_terms_ids.get(term1)\n",
    "    except:\n",
    "        print(\"Term 1 doesn't exist\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        index2 = broadcasted_terms_ids.get(term2)\n",
    "    except IndexError:\n",
    "        print('Term 2 does not exist')\n",
    "        return\n",
    "    \n",
    "    # vector with one non-zero entry (term idf)\n",
    "    term_vector = np.zeros((len(l_idfs),))\n",
    "    term_vector[index1] = l_idfs[index1][1]\n",
    "    term_vector[index2] = l_idfs[index2][1]\n",
    "    \n",
    "    # get original row from V\n",
    "    row = np.dot(term_vector, V)\n",
    "    \n",
    "    # get row\n",
    "    row = np.dot(term_vector, V)\n",
    "    \n",
    "    # get SV\n",
    "    row = np.dot(S, row)\n",
    "    \n",
    "    cosine_sim = np.dot(U, row)\n",
    "    \n",
    "    indices = np.argsort(cosine_sim).tolist()\n",
    "    indices.reverse()\n",
    "    \n",
    "    return list(zip(np_titles[indices[:10]], cosine_sim[indices]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('USS', 0.038413293745060272), ('CDP', 0.02713847552955647), ('USS Adirondack', 0.021318501488897919), ('USS Randolph', 0.020609249300116855), ('USS Intrepid', 0.020413701949279266), ('USS Eel', 0.020282411561892772), ('USS Norfolk', 0.020177801124196849), ('USS Pueblo', 0.020109936796160504), ('USS Raleigh', 0.020083393053348698), ('USS Savannah', 0.020065702228895768)]\n",
      "-----\n",
      "[('Bilinear', 0.024044916207816316), ('Bilinear transformation', 0.012579352824499916), ('Denham', 0.0054376893418000144), ('Partial', 0.0043001705367694629), ('Republican Party', 0.0035980139092924826), ('Unionist Party', 0.0035364045740243722), ('Union Party', 0.0035311054704769613), ('John Denham', 0.0035296391200278182), ('Progressive Party', 0.0035236510674512326), ('Liberty Party', 0.0033634673320921031)]\n"
     ]
    }
   ],
   "source": [
    "print(topDocsFromTermQuery('obama', 'states'))\n",
    "print('-----')\n",
    "print(topDocsFromTermQuery('factorization', 'matrix'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
